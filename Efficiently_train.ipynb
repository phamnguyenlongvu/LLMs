{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrUdZGZweCf/jPrmYe7aSN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phamnguyenlongvu/LLMs/blob/main/Efficiently_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install libraries"
      ],
      "metadata": {
        "id": "kGmIsxdlFqRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP2bTLUB8v0e"
      },
      "outputs": [],
      "source": [
        "!pip install \"peft==0.2.0\"\n",
        "!pip install \"transformers==4.27.2\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" \"bitsandbytes\" loralib --upgrade --quiet\n",
        "!pip install rouge-score tensorboard py7zr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "5xBadBfu9xol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset\n",
        "We will use samsum dataset. This dataset contains about 16k messager - like with summary.\n",
        "\n",
        "Dataset fields:\n",
        "+ Dialogue: text of dialogue\n",
        "+ Summary: human written summary of the dialogue\n",
        "+ id: unique id of an example\n",
        "\n",
        "I will use 5000 samples in train dataset for this demo."
      ],
      "metadata": {
        "id": "GC-JKkfOF3qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")\n",
        "\n",
        "train_dataset = dataset[\"train\"].select(range(5000))\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "id": "s8CxjLrk9IUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = 1\n",
        "print(dataset[\"train\"][x])"
      ],
      "metadata": {
        "id": "sNuaGxjg-Ad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "YAW6yzViG7Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id = \"google/flan-t5-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "uPHC9dQg-Phq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "otNXUY1H-jke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "Summarization is a test-generation task. Model will take a text as input and generate a summary as output. We need to know how long our input and output will take to batch our data efficiently."
      ],
      "metadata": {
        "id": "J23xIDuZHiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "\n",
        "tokenizer_inputs = concatenate_datasets([train_dataset, dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "input_lengths = [len(x) for x in tokenizer_inputs[\"input_ids\"]]\n",
        "max_source_length = int(np.percentile(input_lengths, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "tokenizer_tagets = concatenate_datasets([train_dataset, dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "target_lengths = [len(x) for x in tokenizer_tagets[\"input_ids\"]]\n",
        "max_target_length = int(np.percentile(target_lengths, 90))\n",
        "print(f\"Max target length: {max_target_length}\")\n"
      ],
      "metadata": {
        "id": "hltBIgKs-xt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(sample, padding=\"max_length\"):\n",
        "  inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "  labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "  if padding == \"max_length\":\n",
        "    labels[\"input_ids\"] = [\n",
        "        [(i if i != tokenizer.pad_token_id else -100) for i in label] for label in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs\n",
        "\n",
        "tokenizer_dataset_train = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "tokenizer_dataset_test = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "print(f\"Keys of tokenizer dataset: {list(tokenizer_dataset_train.features)}\")\n",
        "\n",
        "tokenizer_dataset_train.save_to_disk(\"data/train\")\n",
        "tokenizer_dataset_test.save_to_disk(\"data/test\")\n"
      ],
      "metadata": {
        "id": "oaaoEO_aAuHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make CUDA_VERSION=122"
      ],
      "metadata": {
        "id": "D8ZWuxjFJMah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "QAan41bvJ9jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda list | grep cuda"
      ],
      "metadata": {
        "id": "VyFH1Jk1KXTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning T5 with Lora"
      ],
      "metadata": {
        "id": "5ZjjxJSeH4Jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# model_id = \"philschmid/flan-t5-xxl-sharded-fp16\"\n",
        "model_id = \"google/flan-t5-large\"\n",
        "\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "VWrt7IBYDhbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LoRA - Low rank Adaptation\n",
        "- Là một kí thuật học sâu giúp giảm số lượng tham số cần huấn luyện trong mô hình LLMs. Thay vì tinh chỉnh toàn bộ, LoRA chỉ điều chỉnh trọng số của một số ma trận hạng thấp được thêm vào mô hình."
      ],
      "metadata": {
        "id": "jVeg5PuSTAY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=4, # Kích thước của ma trận hạng thấp.\n",
        "    lora_alpha=16, # Hệ số mở rộng (scaling factor)\n",
        "    target_modules=[\"q\", \"v\"], # Modules mà LoRA áp dụng - query, value\n",
        "    lora_dropout=0.01, # Tỉ lệ dropout để tránh overfit\n",
        "    bias=\"none\", # Cách xứ lý bias\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # Xác định loại bài toán\n",
        ")\n",
        "\n",
        "model = prepare_model_for_int8_training(model)\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "y0YgCA7eD0lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "label_pad_token_id= -100\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "706PHriXEt2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "output_dir = \"lora-flan-t5-xxl\"\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1000,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenizer_dataset_train\n",
        ")\n"
      ],
      "metadata": {
        "id": "J7VPDOo5FMuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "LBFBwJMa20Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "LI_YzjFr27dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_id=\"T5-LARGE\"\n",
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)"
      ],
      "metadata": {
        "id": "_Fbvm37hMe_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"T5-LARGE\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded\")"
      ],
      "metadata": {
        "id": "SpSXZy7nL9N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "sample = dataset[\"test\"][randrange(len(dataset[\"test\"]))]\n",
        "\n",
        "input_id = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids\n",
        "\n",
        "outputs = model.generate(input_ids=input_id, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "print(f\"Input sentence: {sample['dialogue']}\")\n",
        "print(f\"\\n {'-' * 30}\")\n",
        "print(f\"Baseline human summary: {sample['summary']}\")\n",
        "print(f\"\\n {'-' * 30}\")\n",
        "print(f\"Model generated summary: {tokenizer.batch_decode(outputs.detach(), skip_special_tokens=True)[0]}\")"
      ],
      "metadata": {
        "id": "BFt8PpWnM4nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_peft_model(sample,max_target_length=50):\n",
        "    # generate summary\n",
        "    outputs = model.generate(input_ids=torch.tensor(sample[\"input_ids\"]).unsqueeze(0), do_sample=True, top_p=0.9, max_new_tokens=max_target_length)\n",
        "    prediction = tokenizer.decode(outputs[0].detach(), skip_special_tokens=True)\n",
        "    # decode eval sample\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(torch.tensor(sample['labels']) != -100, torch.tensor(sample['labels']), tokenizer.pad_token_id)\n",
        "    labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    return prediction, labels\n",
        "\n",
        "eval_dataset = dataset[\"train\"].select(range(5000, 5030))\n",
        "tokenizer_dataset_eval = eval_dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "\n",
        "print(list(tokenizer_dataset_eval.features))\n",
        "\n",
        "# tokenizer_dataset_eval[\"input_ids\"] = torch.tensor(tokenizer_dataset_eval[\"input_ids\"], dtype=torch.long)\n",
        "# tokenizer_dataset_eval[\"attention_mask\"] = torch.tensor(tokenizer_dataset_eval[\"attention_mask\"], dtype=torch.long)\n",
        "# tokenizer_dataset_eval[\"labels\"] = torch.tensor(tokenizer_dataset_eval[\"labels\"], dtype=torch.long)\n",
        "\n",
        "\n",
        "predictions, references = [] , []\n",
        "for sample in tqdm(tokenizer_dataset_eval):\n",
        "    p,l = evaluate_peft_model(sample)\n",
        "    predictions.append(p)\n",
        "    references.append(l)\n",
        "\n",
        "# compute metric\n",
        "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "\n",
        "# print results\n",
        "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
        "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
        "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
        "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")"
      ],
      "metadata": {
        "id": "vQbi-0ZrO1PA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}