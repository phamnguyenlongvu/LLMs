{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPL5VBDrrosBUVVDv3zXQ0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phamnguyenlongvu/LLMs/blob/main/LLM_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction:\n",
        "RAG - Retrieval Augmented Generation allow us to ask questions about our documents (that were not included in the training data), without fine-tuning the LLM. When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed."
      ],
      "metadata": {
        "id": "T4B-dgZmiwxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyjD0cw-0Q0-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain # Framework designed to simplify the creation of applications using LLMS\n",
        "!pip install chromadb # Vector database\n",
        "!pip install sentence_transformers\n",
        "!pip install pypdf\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes #\n",
        "!pip install langchain_community\n",
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Retrieval Augmented Generation (RAG)?\n",
        "LLMs has proven their ability to understand context and provide accurate answers to various NLP tasks, such as summarization, Q&A, text generation, ... While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\" - were not included in their training data. RAG combines external resources with LLMs. Two main components of RAG are a retrieval and a generation.\n",
        "\n",
        "The retriever part can be able to encode our data, so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings - model trained to create a vector representation of the information. The best option for implementing a retriver is a vector database. There are muliple options like ChromaDB, FAISS, Pinecone.\n",
        "\n",
        "The Generator part, the obvious option is LLM."
      ],
      "metadata": {
        "id": "ayyehNYLkGT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from time import time\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "YoOEiEIKD4XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loggin Hugging Face"
      ],
      "metadata": {
        "id": "jsmibm31tBGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "-ntCms3CtFKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize model, tokenizer, quantization and query pipeline\n",
        "Define bitsandbytes configuration.\n",
        "\n",
        "Quantization is a compression technique that involes mapping high precision values to lower precision one. For an LLM, that means modifying the precision of their weights and activations making it less memory intentive. This surely dose have impact on the capabilities of the model including the accuracy.\n",
        "\n",
        "Instead of using high-precision data types, such sa 32-bit floating-point numbers, quantization represents values using lower precision data types. such as 8-bit integers. This process significantly reduces memory usage and can speed up model execution while maintaining acceptable accuracy.\n",
        "\n",
        "- Load a Model in 4-bit or 8-bit Quantization. (print(model.get_memmory.footprint())\n",
        "\n",
        "- Changing the Compute Data Type: bnb_4bit_compute_dtype = torch.bfloat16\n",
        "\n",
        "- Using NF4 Data Type: Desiged for weights initilized using normal distribution\n",
        "\n",
        "- Nest Quantization: bnb_4but_use_double_quant = True\n",
        "\n",
        "- Offloading Between CPU and GPU: llm_int8_enable_fp32_cpu_offload=True\n",
        "\n"
      ],
      "metadata": {
        "id": "b-C2C_kctLFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Load model in 4-bit, you can reduce memory usage by approximately fourfold\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "PUk7SisbFpIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model and tokenizer"
      ],
      "metadata": {
        "id": "LxJtmAS01wQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_start = time()\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    max_new_tokens=1024\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "time_end = time()\n",
        "print(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")"
      ],
      "metadata": {
        "id": "Fxeo0BCuEjbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the query pipeline"
      ],
      "metadata": {
        "id": "av3sqdQX16WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_start = time()\n",
        "query_pipeline = transformers.pipeline(\n",
        "        \"text-generation\", # Task for pipeline\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.float16,\n",
        "        max_length=1024,\n",
        "        device_map=\"auto\",)\n",
        "time_end = time()\n",
        "print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")"
      ],
      "metadata": {
        "id": "5MEhvR3_ZHyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(tokenizer, pipeline, message):\n",
        "    time_start = time()\n",
        "    sequences = pipeline(\n",
        "        message,\n",
        "        do_sample=True, # Enables sampling - True if want creative\n",
        "        top_k=10, # Limits the sampling pool to the top K tokens with the highest prob at each steps\n",
        "        num_return_sequences=1, # Specifies the number of output sequences to generate for each input\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=200) # Maximum numbers of tokens in the generate sequence\n",
        "    time_end = time()\n",
        "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
        "\n",
        "    question = sequences[0]['generated_text'][:len(message)]\n",
        "    answer = sequences[0]['generated_text'][len(message):]\n",
        "\n",
        "    return f\"Question: {question}\\nAnswer: {answer}\\nTotal time: {total_time}\""
      ],
      "metadata": {
        "id": "Z-GHrG1wZfN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Reasoning\", \"Question\", \"Answer\", \"Total time\"], [\"blue\", \"red\", \"green\", \"magenta\"]):\n",
        "        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "xkylYHubZnLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = test_model(tokenizer,\n",
        "                    query_pipeline,\n",
        "                   \"Please explain what is EU AI Act.\")\n",
        "display(Markdown(colorize_text(response)))"
      ],
      "metadata": {
        "id": "N5p0JhkyZp1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Pipeline"
      ],
      "metadata": {
        "id": "3E_eSDTjmHR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
        "\n",
        "\n",
        "time_start = time()\n",
        "question = \"Please explain what EU AI Act is.\"\n",
        "response = llm(prompt=question)\n",
        "time_end = time()\n",
        "total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
        "full_response =  f\"Question: {question}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
        "display(Markdown(colorize_text(full_response)))"
      ],
      "metadata": {
        "id": "HLxK3tsmZxi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load document"
      ],
      "metadata": {
        "id": "JPCvFtZ-lmiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/aiact_final_draft.pdf\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "8uKmQcn8aNBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data in chunks\n",
        "After loaded data, the next step in the indexing pipeling is splittting the documents into manageable chunks.Because:\n",
        "- Easy of search: Large chunks of data are harder to seach over.\n",
        "- Context window size: LLM allow only a finite number of tokens in prompts and completions.\n",
        "\n",
        "Chunking strategies depends on:\n",
        "- Nature of content\n",
        "- Embedding model being used\n",
        "- Expected length and complexity of user queries\n",
        "- Application Specific Requirements\n",
        "\n",
        "Levels of text splitting:\n",
        "- Character Splitting: Simply dividing your text into N-character. It easy, but it don't take into account the structure of our document.\n",
        "- Recursive Character Text Splitter: We'll specify a series of separatators with will be used to split our docs\n",
        "\n"
      ],
      "metadata": {
        "id": "yu16sKwM58tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "93_w04pIaQMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding"
      ],
      "metadata": {
        "id": "rlsf9vh29Gw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ],
      "metadata": {
        "id": "G8nc8OhSaUBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector databases:\n",
        "Using high-dimentional vectors which can contain hundreds of different dimensions (vector index, vector search):\n",
        "\n",
        "- Chroma: Designed for managing and searching color data such as computer vision and image processing.\n",
        "- Milvus\n",
        "- Weaviate\n",
        "- FAISS\n",
        "\n",
        "The choice betwen ChromaDB and FAISS depends on the nature of data and specific requirements of application.\n",
        "- Color-based similarity search -> ChromaDB\n",
        "- General-purpose for similarity search on large-scale vetor data -> FAISS"
      ],
      "metadata": {
        "id": "0kdJs-Ft-kp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
      ],
      "metadata": {
        "id": "0ztuI0ZJaX-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define retriever"
      ],
      "metadata": {
        "id": "lkc4Yveyu1xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "k0zenilTabUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag(qa, query):\n",
        "\n",
        "    time_start = time()\n",
        "    doc = vectordb.similarity_search(query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Retrieved documents: {len(doc)}\")\n",
        "\n",
        "    response = qa.run(query)\n",
        "    time_end = time()\n",
        "    total_time = f\"{round(time_end-time_start, 3)} sec.\"\n",
        "\n",
        "    full_response =  f\"Question: {query}\\nAnswer: {response}\\nTotal time: {total_time}\"\n",
        "    display(Markdown(colorize_text(full_response)))"
      ],
      "metadata": {
        "id": "nwd9k43sadmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How is performed the testing of high-risk AI systems in real world conditions?\"\n",
        "test_rag(qa, query)"
      ],
      "metadata": {
        "id": "fPRF11-PaiD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the operational obligations of notified bodies?\"\n",
        "test_rag(qa, query)"
      ],
      "metadata": {
        "id": "EzT39lImak1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = vectordb.similarity_search(query)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Retrieved documents: {len(docs)}\")\n",
        "for doc in docs:\n",
        "    doc_details = doc.to_json()['kwargs']\n",
        "    print(\"Source: \", doc_details['metadata']['source'])\n",
        "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
      ],
      "metadata": {
        "id": "X1vKPur0auGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}